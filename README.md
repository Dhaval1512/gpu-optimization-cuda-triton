ğŸ“˜CUDA and OpenAI Triton Framwork Implementation
A Research-Driven Performance Analysis of Custom GPU Operators Integrated into a CNN Model (MNIST)

University of Windsor â€” School of Computer Science (2025)
Project Partner: Jaguar Land Rover Canada

ğŸŒ Abstract

Deep learning operations such as GELU, Swish, and Layer Normalization introduce heavy memory traffic and computation overhead in neural networks. This project focuses on optimizing these operations using custom CUDA kernels and Triton kernels, with the goal of achieving performance advantages over standard PyTorch implementations.

We compare:

A PyTorch baseline CNN

A CUDA-accelerated CNN using custom fused operators

A Triton-accelerated CNN using auto-tuned high-performance kernels

Benchmarking reveals that CUDA and Triton significantly outperform PyTorch, achieving up to 6Ã— speedup at lower batch sizes and consistent performance wins across larger batch sizes.

This repository includes the full kernel implementations, CNN model integration, benchmarking pipeline, and visualization tools.

ğŸ§© 1. Motivation

Modern neural networks rely heavily on specialized GPU kernels for performance.
However:

PyTorch uses general-purpose kernels, not optimized for specific tensor shapes.

Many operations (GELU, LN, Swish) involve multiple kernel launches causing extra memory movement.

For small and medium-sized networks (e.g., MNIST CNN), kernel launch overhead becomes a bottleneck.

To address these issues, we implement custom, shape-specialized, memory-efficient fused kernels using:

âœ” CUDA C++

â†’ Full control over threads, memory hierarchy, shared memory, and kernel fusion.

âœ” Triton

â†’ A higher-level GPU DSL built by OpenAI for writing optimized GPU kernels with less complexity.

ğŸš€ 2. Project Objectives
Core Goals

Implement custom GPU operators:

GELU

Swish

LayerNorm

Fused LayerNorm + GELU

Build:

PyTorch CNN (baseline)

CUDA CNN (custom activations)

Triton CNN (custom activations)

Benchmark inference latency across batch sizes.

Compare CUDA vs Triton vs PyTorch.

Visualize latency, speedup, and efficiency.

Enable Nsight Systems & Nsight Compute profiling (for partner evaluation).

Key Outcomes

CUDA achieves 5Ã—â€“6Ã— speedup for small batch sizes.

Triton achieves stable performance and becomes faster for larger batch sizes.

Both significantly outperform PyTorch in all configurations.

ğŸ›ï¸ 3. Repository Structure
gpu-optimization-cuda-triton/
â”‚
â”œâ”€â”€ baseline_cnn_mnist/
â”‚   â”œâ”€â”€ model.py                # Baseline PyTorch CNN
â”‚   â”œâ”€â”€ model_cuda.py           # CNN using custom CUDA activations
â”‚   â”œâ”€â”€ model_triton.py         # CNN using Triton activations
â”‚   â”œâ”€â”€ activations_cuda.py     # CUDA â†’ PyTorch glue code
â”‚   â”œâ”€â”€ activations_triton.py   # Triton â†’ PyTorch glue code
â”‚   â”œâ”€â”€ train.py                # Baseline training script
â”‚   â””â”€â”€ inference_benchmark.py  # PyTorch vs CUDA vs Triton benchmark
â”‚
â”œâ”€â”€ cuda_kernels/               # All CUDA implementations
â”‚   â”œâ”€â”€ gelu.cu
â”‚   â”œâ”€â”€ swish.cu
â”‚   â”œâ”€â”€ layernorm.cu
â”‚   â”œâ”€â”€ fused_layernorm_gelu.cu
â”‚   â””â”€â”€ cuda_ops_all.cu         # Combined kernel file
â”‚
â”œâ”€â”€ triton_kernels/             # Triton implementations
â”‚   â”œâ”€â”€ gelu.py
â”‚   â”œâ”€â”€ swish.py
â”‚   â”œâ”€â”€ layernorm.py
â”‚   â”œâ”€â”€ fused_layernorm_gelu.py
â”‚   â””â”€â”€ triton_ops.py           # Combined Triton dispatch
â”‚
â”œâ”€â”€ extensions/                 # Python-CUDA bindings
â”‚   â””â”€â”€ cuda_ops.py
â”‚
â”œâ”€â”€ plots/                      # Graph generation scripts
â”œâ”€â”€ benchmarks/                 # Kernel microbenchmarks
â”œâ”€â”€ report/                     # Analysis scripts
â”‚
â”œâ”€â”€ setup.py                    # Build CUDA extension
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”¬ 4. CNN Architecture (MNIST Baseline)

A lightweight CNN is used to clearly measure the effect of kernel optimization:

Input (1Ã—28Ã—28)
â†’ Conv(1â†’16) + ReLU / CUDA Swish / Triton Swish
â†’ MaxPool
â†’ Conv(16â†’32) + ReLU / CUDA GELU / Triton GELU
â†’ MaxPool
â†’ Flatten
â†’ Fully Connected (32*7*7 â†’ 128)
â†’ Fully Connected (128 â†’ 10)
â†’ Softmax

âš™ï¸ 5. Custom CUDA Kernels (C++/CUDA)

Each kernel is implemented with:

grid-stride loops

shared memory usage

warp-level parallelism

reduction for LN

fused execution to minimize memory access

Example (Fused LayerNorm + GELU):
// pseudo-code
mean = reduce_sum(x) / C;
var  = reduce_sum((x - mean)^2) / C;
norm = (x - mean) / sqrt(var + eps);
y = 0.5 * (norm * gamma + beta) * (1 + erf(...));


Exported into PyTorch via C++ extension.

âš¡ 6. Triton Kernels

Triton kernels leverage:

program_id indexing

block-level memory loads

auto-tuned BLOCK_SIZE

vectorized math

warp-aware reductions

Example (Triton GELU):

@triton.jit
def gelu_kernel(X, Y, N, BLOCK: tl.constexpr):
    off = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)
    x = tl.load(X + off, mask=off < N)
    y = 0.5 * x * (1 + tl.erf(x * 0.707106))
    tl.store(Y + off, y, mask=off < N)

ğŸ§ª 7. Benchmarking Methodology
Batch sizes tested
16, 32, 64, 128

Metrics

End-to-end inference latency (ms/batch)

GPU synchronization for accurate measurement

Repeated inference to stabilize timing

ğŸ“Š 8. Performance Results
Latency (ms per batch)
Batch Size	PyTorch	CUDA	Triton
16	2.1791	0.3530	1.8914
32	0.4270	0.3646	0.4211
64	0.4219	0.4238	0.3977
128	0.7777	0.7059	0.6969
Speedup (vs PyTorch)
Batch Size	CUDA	Triton
16	6.16Ã—	1.15Ã—
32	1.17Ã—	1.01Ã—
64	0.99Ã—	1.06Ã—
128	1.10Ã—	1.11Ã—
ğŸ“‰ 9. Graphs & Visualization

Run:

python plots/plot_inference_results.py
python plots/plot_more_graphs.py


Produces:

Latency Comparison (Line + Bar)

Speedup Comparison

Efficiency Score

Scatter Trends

Relative Performance Normalized

All plots are saved in plots/ folder.

ğŸ” 10. GPU Profiling (Nsight Systems / Compute)

Your friend (or any machine with NVIDIA GPU + Nsight) can profile using:

Nsight Systems
nsys profile -o profiling/cnn_run python baseline_cnn_mnist/inference_benchmark.py

Nsight Compute
ncu --set full python baseline_cnn_mnist/inference_benchmark.py


Collect:

SM utilization

Warp occupancy

DRAM throughput

Kernel execution timeline

ğŸ 11. Key Findings

CUDA kernels provide significant low-batch speedup
Kernel fusion and reduced memory access lead to 6Ã— improvement.

Triton kernels scale better at larger batches
Due to auto-tuned tile sizes and better memory scheduling.

Both CUDA & Triton outperform PyTorch consistently
Even without deep kernel-level tuning.

LayerNorm & GELU are the dominant bottlenecks in small CNNs
Custom fused kernels reduce launch overhead.

ğŸ› ï¸ 12. Setup Instructions
Install dependencies:
pip install -r requirements.txt

Build CUDA kernels:
python setup.py build_ext --inplace

Train model:
python baseline_cnn_mnist/train.py

Benchmark:
python baseline_cnn_mnist/inference_benchmark.py

ğŸ” 13. Future Work

Deeper kernel fusion (Conv+BN+ReLU)

Transformer-style fused attention kernels

Shared-memory optimized LayerNorm

Warp-specialized Triton kernels

Full Nsight Compute analysis

Cross-hardware benchmarking (RTX vs A100)

ğŸ¤ 14. Contributors & Acknowledgements

Team Members:

Dhaval Patel

Kunal Panchal

Rutesh Zalavadiya

University of Windsor CS Students

Industry Partner:

Jaguar Land Rover Canada â€” GPU Optimization Initiative

Supervision:

School of Computer Science, University of Windsor

ğŸ“„ 15. License

This project is for educational & research use only.
